{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random, numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense(\n",
      "    # attributes\n",
      "    features = 5\n",
      "    use_bias = True\n",
      "    dtype = None\n",
      "    param_dtype = float32\n",
      "    precision = None\n",
      "    kernel_init = init\n",
      "    bias_init = zeros\n",
      "    dot_general = None\n",
      "    dot_general_cls = None\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Dense(features=5)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'bias': (5,), 'kernel': (10, 5)}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key1, key2 = random.split(random.key(0))\n",
    "x = random.normal(key1, (10,))  # dummy input\n",
    "params = model.init(key2, x)\n",
    "\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([-1.3721199 ,  0.611315  ,  0.64428365,  2.2192967 , -1.1271119 ],      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.apply(params, x)\n",
    "\n",
    "print(type(result))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| x_samples.shape: (20, 10)\n",
      "ic| y_samples.shape: (20, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from icecream import ic\n",
    "\n",
    "# set problem dimensions\n",
    "n_samples = 20\n",
    "x_dim = 10\n",
    "y_dim = 5\n",
    "\n",
    "key = random.key(0)\n",
    "k1, k2 = random.split(key)\n",
    "W = random.normal(k1, (x_dim, y_dim))\n",
    "b = random.normal(k2, (y_dim,))\n",
    "\n",
    "true_params = flax.core.freeze({\"params\": {\"bias\": b, \"kernel\": W}})\n",
    "\n",
    "\n",
    "# Generate Samples with noise\n",
    "\n",
    "key_sample, key_noise = random.split(k1)\n",
    "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
    "y_samples = (\n",
    "    jnp.dot(x_samples, W) + b + 0.1 * random.normal(key_noise, (n_samples, y_dim))\n",
    ")\n",
    "\n",
    "ic(x_samples.shape)\n",
    "ic(y_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def mse(params, x_batched, y_batched):\n",
    "    # define the squared error loss\n",
    "    def squared_error(x, y):\n",
    "        pred = model.apply(params, x)\n",
    "        return jnp.inner(y - pred, y - pred) / 2\n",
    "\n",
    "    return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.0236398, dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.3\n",
    "mse(true_params, x_samples, y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss 35.343875885009766\n",
      "Step 11, Loss 0.5143467783927917\n",
      "Step 21, Loss 0.11384160816669464\n",
      "Step 31, Loss 0.039326731115579605\n",
      "Step 41, Loss 0.01991620473563671\n",
      "Step 51, Loss 0.014209133572876453\n",
      "Step 61, Loss 0.012425646185874939\n",
      "Step 71, Loss 0.01185037661343813\n",
      "Step 81, Loss 0.011661776341497898\n",
      "Step 91, Loss 0.011599412187933922\n",
      "Step 101, Loss 0.01157870702445507\n"
     ]
    }
   ],
   "source": [
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_params(params, learning_rate, grads):\n",
    "    params = jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, params, grads)\n",
    "    return params\n",
    "\n",
    "\n",
    "for i in range(101):\n",
    "    loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "    params = update_params(params, learning_rate, grads)  # type: ignore\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Step {i+1}, Loss {loss_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss 0.011577614583075047\n",
      "Step 11, Loss 0.26143109798431396\n",
      "Step 21, Loss 0.07674869149923325\n",
      "Step 31, Loss 0.03644001856446266\n",
      "Step 41, Loss 0.022012466564774513\n",
      "Step 51, Loss 0.016178492456674576\n",
      "Step 61, Loss 0.013002862222492695\n",
      "Step 71, Loss 0.01202611718326807\n",
      "Step 81, Loss 0.011764513328671455\n",
      "Step 91, Loss 0.011646024882793427\n",
      "Step 101, Loss 0.011585516855120659\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "\n",
    "tx = optax.adam(learning_rate=learning_rate)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "for i in range(101):\n",
    "    loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Step {i+1}, Loss {loss_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| dict_output: {'params': {'bias': Array([-1.4555763 , -2.027799  ,  2.0790977 ,  1.2186146 , -0.99809825],      dtype=float32),\n",
      "                             'kernel': Array([[ 1.0098811 ,  0.18934365,  0.04455001, -0.92802244,  0.34784022],\n",
      "                        [ 1.7298453 ,  0.987937  ,  1.1640465 ,  1.1006079 , -0.10653906],\n",
      "                        [-1.2029463 ,  0.28635174,  1.415598  ,  0.11870932, -1.3141485 ],\n",
      "                        [-1.1941485 , -0.189585  ,  0.03413848,  1.3169427 ,  0.08060391],\n",
      "                        [ 0.13852438,  1.3713042 , -1.3187189 ,  0.53152686, -2.2404995 ],\n",
      "                        [ 0.56294   ,  0.81223136,  0.31752002,  0.5345511 ,  0.90500396],\n",
      "                        [-0.37926036,  1.7410394 ,  1.079029  , -0.5039834 ,  0.9283064 ],\n",
      "                        [ 0.97064894, -1.3153405 ,  0.33681473,  0.8099342 , -1.2018456 ],\n",
      "                        [ 1.019431  , -0.6202477 ,  1.0818828 , -1.8389741 , -0.45804927],\n",
      "                        [-0.6436536 ,  0.45666718, -1.1329136 , -0.6853864 ,  0.16828988]],      dtype=float32)}}\n",
      "ic| bytes_output: (b'\\x81\\xa6params\\x82\\xa4bias\\xc7!\\x01\\x93\\x91\\x05\\xa7float32\\xc4\\x14SP'\n",
      "                   b'\\xba\\xbfu\\xc7\\x01\\xc0\\xf0\\x0f\\x05@\\x90\\xfb\\x9b?^\\x83\\x7f\\xbf\\xa6kerne'\n",
      "                   b'l\\xc7\\xd6\\x01\\x93\\x92\n",
      "                  \\x05\\xa7float32\\xc4\\xc8\\xc9C\\x81?M\\xe3A>\\x11z'\n",
      "                   b'6=\\xe1\\x92m\\xbf\\x1d\\x18\\xb2>\\x92k\\xdd?p\\xe9|?z\\xff\\x94?\\xb8\\xe0\\x8c?&1'\n",
      "                   b'\\xda\\xbd%\\xfa\\x99\\xbf\\xb2\\x9c\\x92>Q2\\xb5?\\xdf\\x1d\\xf3=\\x056\\xa8\\xbf\\xdc\\xd9'\n",
      "                   b'\\x98\\xbf\\x92\"B\\xbe\\xcb\\xd4\\x0b=\\x94\\x91\\xa8?\\xaa\\x13\\xa5=V\\xd9\\r>\\xe5\\x86'\n",
      "                   b'\\xaf?\\xc8\\xcb\\xa8\\xbf%\\x12\\x08?Xd\\x0f\\xc0\\xd6\\x1c\\x10?e\\xeeO?\\xfc\\x91'\n",
      "                   b'\\xa2>W\\xd8\\x08?W\\xaeg?j.\\xc2\\xbea\\xda\\xde?\\x9f\\x1d\\x8a?\\x0e\\x05\\x01\\xbf}\\xa5'\n",
      "                   b'm?s|x?\\x14]\\xa8\\xbf\\xfbr\\xac>\\xd9WO?\\x14\\xd6\\x99\\xbf\\xb7|\\x82?\\x8e\\xc8'\n",
      "                   b'\\x1e\\xbf#{\\x8a?\\x81c\\xeb\\xbfo\\x85\\xea\\xbe{\\xc6$\\xbfH\\xd0\\xe9>P\\x03\\x91\\xbf|u'\n",
      "                   b'/\\xbf/T,>')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'\\x81\\xa6params\\x82\\xa4bias\\xc7!\\x01\\x93\\x91\\x05\\xa7float32\\xc4\\x14SP\\xba\\xbfu\\xc7\\x01\\xc0\\xf0\\x0f\\x05@\\x90\\xfb\\x9b?^\\x83\\x7f\\xbf\\xa6kernel\\xc7\\xd6\\x01\\x93\\x92\\n\\x05\\xa7float32\\xc4\\xc8\\xc9C\\x81?M\\xe3A>\\x11z6=\\xe1\\x92m\\xbf\\x1d\\x18\\xb2>\\x92k\\xdd?p\\xe9|?z\\xff\\x94?\\xb8\\xe0\\x8c?&1\\xda\\xbd%\\xfa\\x99\\xbf\\xb2\\x9c\\x92>Q2\\xb5?\\xdf\\x1d\\xf3=\\x056\\xa8\\xbf\\xdc\\xd9\\x98\\xbf\\x92\"B\\xbe\\xcb\\xd4\\x0b=\\x94\\x91\\xa8?\\xaa\\x13\\xa5=V\\xd9\\r>\\xe5\\x86\\xaf?\\xc8\\xcb\\xa8\\xbf%\\x12\\x08?Xd\\x0f\\xc0\\xd6\\x1c\\x10?e\\xeeO?\\xfc\\x91\\xa2>W\\xd8\\x08?W\\xaeg?j.\\xc2\\xbea\\xda\\xde?\\x9f\\x1d\\x8a?\\x0e\\x05\\x01\\xbf}\\xa5m?s|x?\\x14]\\xa8\\xbf\\xfbr\\xac>\\xd9WO?\\x14\\xd6\\x99\\xbf\\xb7|\\x82?\\x8e\\xc8\\x1e\\xbf#{\\x8a?\\x81c\\xeb\\xbfo\\x85\\xea\\xbe{\\xc6$\\xbfH\\xd0\\xe9>P\\x03\\x91\\xbf|u/\\xbf/T,>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flax import serialization\n",
    "\n",
    "bytes_output = serialization.to_bytes(params)\n",
    "dict_output = serialization.to_state_dict(params)\n",
    "\n",
    "ic(dict_output)\n",
    "ic(bytes_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'bias': array([-1.4555763 , -2.027799  ,  2.0790977 ,  1.2186146 , -0.99809825],\n",
       "        dtype=float32),\n",
       "  'kernel': array([[ 1.0098811 ,  0.18934365,  0.04455001, -0.92802244,  0.34784022],\n",
       "         [ 1.7298453 ,  0.987937  ,  1.1640465 ,  1.1006079 , -0.10653906],\n",
       "         [-1.2029463 ,  0.28635174,  1.415598  ,  0.11870932, -1.3141485 ],\n",
       "         [-1.1941485 , -0.189585  ,  0.03413848,  1.3169427 ,  0.08060391],\n",
       "         [ 0.13852438,  1.3713042 , -1.3187189 ,  0.53152686, -2.2404995 ],\n",
       "         [ 0.56294   ,  0.81223136,  0.31752002,  0.5345511 ,  0.90500396],\n",
       "         [-0.37926036,  1.7410394 ,  1.079029  , -0.5039834 ,  0.9283064 ],\n",
       "         [ 0.97064894, -1.3153405 ,  0.33681473,  0.8099342 , -1.2018456 ],\n",
       "         [ 1.019431  , -0.6202477 ,  1.0818828 , -1.8389741 , -0.45804927],\n",
       "         [-0.6436536 ,  0.45666718, -1.1329136 , -0.6853864 ,  0.16828988]],\n",
       "        dtype=float32)}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialization.from_bytes(params, bytes_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
      "output:\n",
      " [[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.00723789 -0.00810346 -0.02550935  0.02151712 -0.01261239]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Callable, Sequence\n",
    "\n",
    "\n",
    "class ExplicitMLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "\n",
    "    def setup(self):\n",
    "        # we automatically know what to do with lists, dicts of submodules\n",
    "        self.layers = [nn.Dense(feat) for feat in self.features]\n",
    "        # for single submodules, we would just write:\n",
    "        # self.layer1 = nn.Dense(feat1)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for i, lyr in enumerate(self.layers):\n",
    "            x = lyr(x)\n",
    "            if i != len(self.layers) - 1:\n",
    "                x = nn.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "key1, key2 = random.split(random.key(0), 2)\n",
    "x = random.uniform(key1, (4, 4))\n",
    "\n",
    "model = ExplicitMLP(features=[3, 4, 5])\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print(\n",
    "    \"initialized parameter shapes:\\n\",\n",
    "    jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(params)),\n",
    ")\n",
    "print(\"output:\\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we’ll implement a simplified but similar mechanism to batch normalization: we’ll store running averages and subtract those to the input at training time. For proper batchnorm, you should use (and look at) the implementation [here](https://github.com/google/flax/blob/main/flax/linen/normalization.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized variables:\n",
      " {'batch_stats': {'mean': Array([0., 0., 0., 0., 0.], dtype=float32)}, 'params': {'bias': Array([0., 0., 0., 0., 0.], dtype=float32)}}\n",
      "updated state:\n",
      " {'batch_stats': {'mean': Array([[0.01, 0.01, 0.01, 0.01, 0.01]], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "class BiasAdderWithRunningMean(nn.Module):\n",
    "    decay: float = 0.99\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # easy pattern to detect if we're initializing via empty variable tree\n",
    "        is_initialized = self.has_variable(\"batch_stats\", \"mean\")\n",
    "        ra_mean = self.variable(\n",
    "            \"batch_stats\", \"mean\", lambda s: jnp.zeros(s), x.shape[1:]\n",
    "        )\n",
    "        bias = self.param(\"bias\", lambda rng, shape: jnp.zeros(shape), x.shape[1:])\n",
    "        if is_initialized:\n",
    "            ra_mean.value = self.decay * ra_mean.value + (1.0 - self.decay) * jnp.mean(\n",
    "                x, axis=0, keepdims=True\n",
    "            )\n",
    "\n",
    "        return x - ra_mean.value + bias\n",
    "\n",
    "\n",
    "key1, key2 = random.split(random.key(0), 2)\n",
    "x = jnp.ones((10, 5))\n",
    "model = BiasAdderWithRunningMean()\n",
    "\n",
    "variables = model.init(key1, x)\n",
    "print(\"initialized variables:\\n\", variables)\n",
    "\n",
    "y, updated_state = model.apply(variables, x, mutable=[\"batch_stats\"])\n",
    "print(\"updated state:\\n\", updated_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated state:\n",
      " {'batch_stats': {'mean': Array([[0.01, 0.01, 0.01, 0.01, 0.01]], dtype=float32)}}\n",
      "updated state:\n",
      " {'batch_stats': {'mean': Array([[0.0299, 0.0299, 0.0299, 0.0299, 0.0299]], dtype=float32)}}\n",
      "updated state:\n",
      " {'batch_stats': {'mean': Array([[0.059601, 0.059601, 0.059601, 0.059601, 0.059601]], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "for val in [1.0, 2.0, 3.0]:\n",
    "    x = val * jnp.ones((10, 5))\n",
    "    y, updated_state = model.apply(variables, x, mutable=[\"batch_stats\"])\n",
    "    old_state, params = flax.core.pop(variables, \"params\")\n",
    "    variables = flax.core.freeze({\"params\": params, **updated_state})\n",
    "    print(\"updated state:\\n\", updated_state)  # Shows only the mutable part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

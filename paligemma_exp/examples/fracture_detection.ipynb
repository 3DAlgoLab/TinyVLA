{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install roboflow supervision -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')\n",
    "ROBOFLOW_API_KEY = os.getenv(\"ROBOFLOW_API_KEY\")\n",
    "\n",
    "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
    "project = rf.workspace(\"srinithi-s-tzdkb\").project(\"fracture-detection-rhud5\")\n",
    "version = project.version(4)\n",
    "dataset = version.download(\"paligemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import json\n",
    "import supervision as sv\n",
    "\n",
    "first = json.loads(\n",
    "    open(f\"{dataset.location}/dataset/_annotations.train.jsonl\").readline()\n",
    ")\n",
    "print(first)\n",
    "\n",
    "image = Image.open(f\"{dataset.location}/dataset/{first.get('image')}\")\n",
    "CLASSES = first.get(\"prefix\").replace(\"detect \", \"\").split(\" ; \")\n",
    "# detections = from_pali_gemma(first.get(\"suffix\"), image.size, CLASSES)\n",
    "w, h = image.width, image.height\n",
    "detections = sv.Detections.from_lmm(\n",
    "    lmm=\"paligemma\",\n",
    "    result=first.get(\"suffix\"),\n",
    "    resolution_wh=(w, h),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "\n",
    "sv.BoundingBoxAnnotator().annotate(image, detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# TPUs with\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "    raise \"It seems you are using Colab with remote TPUs which is not supported.\"\n",
    "\n",
    "# Fetch big_vision repository if python doesn't know about it and install\n",
    "# dependencies needed for this notebook.\n",
    "if not os.path.exists(\"big_vision\"):\n",
    "    !git clone --quiet --branch=main --depth=1 \\\n",
    "     https://github.com/google-research/big_vision big_vision_repo\n",
    "    %mv big_vision_repo/big_vision ./\n",
    "    %rm -rf big_vision_repo\n",
    "\n",
    "# Install missing dependencies. Assume jax~=0.4.25 with GPU available.\n",
    "%pip install -q \"overrides\" \"ml_collections\" \"einops~=0.7\" \"sentencepiece\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "MODEL_PATH = \"./pt_224_128.params.f16.npz\"\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(\"Downloading the checkpoint from Kaggle, this could take a few minutes....\")\n",
    "    # Note: kaggle archive contains the same checkpoint in multiple formats.\n",
    "    # Download only the float16 model.\n",
    "    MODEL_PATH = kagglehub.model_download(\n",
    "        \"google/paligemma/jax/paligemma-3b-pt-224\", \"paligemma-3b-pt-224.f16.npz\"\n",
    "    )\n",
    "    print(f\"Model path: {MODEL_PATH}\")\n",
    "\n",
    "\n",
    "TOKENIZER_PATH = \"./paligemma_tokenizer.model\"\n",
    "if not os.path.exists(TOKENIZER_PATH):\n",
    "    print(\"Downloading the model tokenizer...\")\n",
    "    !gsutil cp gs://big_vision/paligemma_tokenizer.model {TOKENIZER_PATH}\n",
    "    print(f\"Tokenizer path: {TOKENIZER_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_collections\n",
    "\n",
    "# Import model definition from big_vision\n",
    "from big_vision.models.proj.paligemma import paligemma\n",
    "from big_vision.trainers.proj.paligemma import predict_fns\n",
    "\n",
    "# Import big vision utilities\n",
    "import big_vision.datasets.jsonl\n",
    "import big_vision.utils\n",
    "import big_vision.sharding\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import tensorflow as tf\n",
    "import sentencepiece\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import functools\n",
    "\n",
    "\n",
    "# Don't let TF use the GPU or TPUs\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "tf.config.set_visible_devices([], \"TPU\")\n",
    "\n",
    "backend = jax.lib.xla_bridge.get_backend()\n",
    "print(f\"JAX version:  {jax.__version__}\")\n",
    "print(f\"JAX platform: {backend.platform}\")\n",
    "print(f\"JAX devices:  {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model_config = ml_collections.FrozenConfigDict(\n",
    "    {\n",
    "        \"llm\": {\"vocab_size\": 257_152},\n",
    "        \"img\": {\n",
    "            \"variant\": \"So400m/14\",\n",
    "            \"pool_type\": \"none\",\n",
    "            \"scan\": True,\n",
    "            \"dtype_mm\": \"float16\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "model = paligemma.Model(**model_config)\n",
    "tokenizer = sentencepiece.SentencePieceProcessor(TOKENIZER_PATH)\n",
    "\n",
    "# Load params - this can take up to 1 minute in T4 colabs.\n",
    "params = paligemma.load(None, MODEL_PATH, model_config)\n",
    "\n",
    "# Define `decode` function to sample outputs from the model.\n",
    "decode_fn = predict_fns.get_all(model)[\"decode\"]\n",
    "decode = functools.partial(\n",
    "    decode_fn, devices=jax.devices(), eos_token=tokenizer.eos_id()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "# Create a pytree mask of the trainable params.\n",
    "def is_trainable_param(name, param):  # pylint: disable=unused-argument\n",
    "    if name.startswith(\"llm/layers/attn/\"):\n",
    "        return True\n",
    "    if name.startswith(\"llm/\"):\n",
    "        return False\n",
    "    if name.startswith(\"img/\"):\n",
    "        return False\n",
    "    raise ValueError(f\"Unexpected param name {name}\")\n",
    "\n",
    "\n",
    "trainable_mask = big_vision.utils.tree_map_with_names(is_trainable_param, params)\n",
    "\n",
    "# If more than one device is available (e.g. multiple GPUs) the parameters can\n",
    "# be sharded across them to reduce HBM usage per device.\n",
    "mesh = jax.sharding.Mesh(jax.devices(), (\"data\"))\n",
    "data_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(\"data\"))\n",
    "params_sharding = big_vision.sharding.infer_sharding(\n",
    "    params, strategy=[(\".*\", 'fsdp(axis=\"data\")')], mesh=mesh\n",
    ")\n",
    "\n",
    "# Yes: Some donated buffers are not usable.\n",
    "warnings.filterwarnings(\"ignore\", message=\"Some donated buffers were not usable\")\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, donate_argnums=(0,), static_argnums=(1,))\n",
    "def maybe_cast_to_f32(params, trainable):\n",
    "    return jax.tree.map(\n",
    "        lambda p, m: p.astype(jnp.float32) if m else p, params, trainable\n",
    "    )\n",
    "\n",
    "\n",
    "# Loading all params in simultaneous - albeit much faster and more succinct -\n",
    "# requires more RAM than the T4 colab runtimes have by default.\n",
    "# Instead we do it param by param.\n",
    "params, treedef = jax.tree.flatten(params)\n",
    "sharding_leaves = jax.tree.leaves(params_sharding)\n",
    "trainable_leaves = jax.tree.leaves(trainable_mask)\n",
    "for idx, (sharding, trainable) in enumerate(zip(sharding_leaves, trainable_leaves)):\n",
    "    params[idx] = big_vision.utils.reshard(params[idx], sharding)\n",
    "    params[idx] = maybe_cast_to_f32(params[idx], trainable)\n",
    "    params[idx].block_until_ready()\n",
    "params = jax.tree.unflatten(treedef, params)\n",
    "\n",
    "\n",
    "# Print params to show what the model is made of.\n",
    "def parameter_overview(params):\n",
    "    for path, arr in big_vision.utils.tree_flatten_with_names(params)[0]:\n",
    "        print(f\"{path:80s} {str(arr.shape):22s} {arr.dtype}\")\n",
    "\n",
    "\n",
    "print(\" == Model params == \")\n",
    "parameter_overview(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model inputs\n",
    "\n",
    "The model checkpoint you're using has already been trained on images of various aspect ratios that have been resized to 224x224 pixels, and to handle tokenized texts.\n",
    "\n",
    "The code below defines three functions that you'll use in the next step create the model's inputs:\n",
    "* **`preprocess_image`:** Normalizes the image data. In this case, pre-processing converts the passed-in image to greyscale, removes the alpha layer, and resizes the passed-in image to the size required by the model for image inputs (224x224 pixels).\n",
    "* **`preprocess_tokens`:** Splits the tokens up and adds flags to mark whether a token is a prefix or suffix token. These flags will be used later on in the code, during the training step and the evaluation loop.\n",
    "* **`postprocess_tokens`:** Removes any tokens left at and/or after the end-of-sequence (EOS) token and returns the remaining decoded tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocess_image(image, size=224):\n",
    "    \"\"\"\n",
    "    Model has been trained to handle images of different aspects ratios\n",
    "    resized to 224x224 in the range [-1, 1]. Bilinear and antialias resize\n",
    "    options are helpful to improve quality in some tasks.\n",
    "    \"\"\"\n",
    "    image = np.asarray(image)\n",
    "    if image.ndim == 2:  # Convert image without last channel into grayscale.\n",
    "        image = np.stack((image,) * 3, axis=-1)\n",
    "    image = image[..., :3]  # Remove alpha layer.\n",
    "    assert image.shape[-1] == 3\n",
    "\n",
    "    image = tf.constant(image)\n",
    "    image = tf.image.resize(image, (size, size), method=\"bilinear\", antialias=True)\n",
    "    return image.numpy() / 127.5 - 1.0  # [0, 255]->[-1,1]\n",
    "\n",
    "\n",
    "def preprocess_tokens(prefix, suffix=None, seqlen=None):\n",
    "    \"\"\"\n",
    "    Model has been trained to handle tokenized text composed of a prefix with\n",
    "    full attention and a suffix with causal attention.\n",
    "    \"\"\"\n",
    "    separator = \"\\n\"\n",
    "    tokens = tokenizer.encode(prefix, add_bos=True) + tokenizer.encode(separator)\n",
    "    mask_ar = [0] * len(tokens)  # 0 to use full attention for prefix.\n",
    "    mask_loss = [0] * len(tokens)  # 0 to not use prefix tokens in the loss.\n",
    "\n",
    "    if suffix:\n",
    "        suffix = tokenizer.encode(suffix, add_eos=True)\n",
    "        tokens += suffix\n",
    "        mask_ar += [1] * len(suffix)  # 1 to use causal attention for suffix.\n",
    "        mask_loss += [1] * len(suffix)  # 1 to use suffix tokens in the loss.\n",
    "\n",
    "    mask_input = [1] * len(tokens)  # 1 if it's a token, 0 if padding.\n",
    "    if seqlen:\n",
    "        padding = [0] * max(0, seqlen - len(tokens))\n",
    "        tokens = tokens[:seqlen] + padding\n",
    "        mask_ar = mask_ar[:seqlen] + padding\n",
    "        mask_loss = mask_loss[:seqlen] + padding\n",
    "        mask_input = mask_input[:seqlen] + padding\n",
    "\n",
    "    return jax.tree.map(np.array, (tokens, mask_ar, mask_loss, mask_input))\n",
    "\n",
    "\n",
    "def postprocess_tokens(tokens):\n",
    "    tokens = tokens.tolist()  # np.array to list[int]\n",
    "    try:  # Remove tokens at and after EOS if any.\n",
    "        eos_pos = tokens.index(tokenizer.eos_id())\n",
    "        tokens = tokens[:eos_pos]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training and validation iterators\n",
    "\n",
    "Create two iterators:\n",
    "\n",
    "*   A **training iterator** to allow the training process to go through the data in chunks rather than processing it all at once\n",
    "    *   This allows you to do some data pre-processing before use\n",
    "*   A **validation iterator** that allows the training process to iterate over the validation dataset to see how well the tuned model aligned with the provided results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "SEQLEN = 128\n",
    "DATA_DIR = dataset.location + \"/dataset\"\n",
    "\n",
    "train_dataset = big_vision.datasets.jsonl.DataSource(\n",
    "    os.path.join(DATA_DIR, \"_annotations.train.jsonl\"), fopen_keys={\"image\": DATA_DIR}\n",
    ")\n",
    "\n",
    "val_dataset = big_vision.datasets.jsonl.DataSource(\n",
    "    os.path.join(DATA_DIR, \"_annotations.valid.jsonl\"), fopen_keys={\"image\": DATA_DIR}\n",
    ")\n",
    "\n",
    "\n",
    "def train_data_iterator():\n",
    "    \"\"\"Never ending iterator over training examples.\"\"\"\n",
    "    # Shuffle examples and repeat so one can train for many epochs.\n",
    "    dataset = train_dataset.get_tfdata().shuffle(1_000).repeat().prefetch(1)\n",
    "    for example in dataset.as_numpy_iterator():\n",
    "        image = Image.open(io.BytesIO(example[\"image\"]))\n",
    "        image = preprocess_image(image)\n",
    "\n",
    "        # prefix = \"caption en\"  # Could also be a different prefix per example.\n",
    "        # suffix = example[\"suffix\"].decode().lower()\n",
    "\n",
    "        prefix = example[\"prefix\"].decode().lower()\n",
    "        suffix = example[\"suffix\"].decode().lower()\n",
    "        tokens, mask_ar, mask_loss, _ = preprocess_tokens(prefix, suffix, SEQLEN)\n",
    "        label, _, _, _ = preprocess_tokens(suffix, seqlen=SEQLEN)\n",
    "\n",
    "        yield {\n",
    "            \"image\": np.asarray(image),\n",
    "            \"text\": np.asarray(tokens),\n",
    "            \"label\": np.asarray(label),\n",
    "            \"mask_ar\": np.asarray(mask_ar),\n",
    "            \"mask_loss\": np.asarray(mask_loss),\n",
    "        }\n",
    "\n",
    "\n",
    "def validation_data_iterator():\n",
    "    \"\"\"Single iterator over validation examples.\"\"\"\n",
    "    dataset = val_dataset.get_tfdata(ordered=True).prefetch(1)\n",
    "    for example in dataset.as_numpy_iterator():\n",
    "        image = Image.open(io.BytesIO(example[\"image\"]))\n",
    "        image = preprocess_image(image)\n",
    "\n",
    "        prefix = example[\"prefix\"].decode().lower()\n",
    "        suffix = example[\"suffix\"].decode().lower()\n",
    "        tokens, mask_ar, _, mask_input = preprocess_tokens(prefix, seqlen=SEQLEN)\n",
    "        label, _, _, _ = preprocess_tokens(suffix, seqlen=SEQLEN)\n",
    "\n",
    "        yield {\n",
    "            \"image\": np.asarray(image),\n",
    "            \"text\": np.asarray(tokens),\n",
    "            \"label\": np.asarray(label),\n",
    "            \"mask_ar\": np.asarray(mask_ar),\n",
    "            \"mask_input\": np.asarray(mask_input),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import html\n",
    "\n",
    "\n",
    "def split_and_keep_second_part(s):\n",
    "    parts = s.split(\"\\n\", 1)\n",
    "    if len(parts) > 1:\n",
    "        return parts[1]\n",
    "    return s\n",
    "\n",
    "\n",
    "def render_inline(image, resize=(128, 128)):\n",
    "    \"\"\"Convert image into inline html.\"\"\"\n",
    "    image = Image.fromarray(image)\n",
    "    image.resize(resize)\n",
    "    with io.BytesIO() as buffer:\n",
    "        image.save(buffer, format=\"jpeg\")\n",
    "        image_b64 = str(base64.b64encode(buffer.getvalue()), \"utf-8\")\n",
    "        return f\"data:image/jpeg;base64,{image_b64}\"\n",
    "\n",
    "\n",
    "def render_example(image, caption):\n",
    "    image = ((image + 1) / 2 * 255).astype(np.uint8)  # [-1,1] -> [0, 255]\n",
    "    h, w, _ = image.shape\n",
    "    try:\n",
    "        detections = sv.Detections.from_lmm(\n",
    "            lmm=\"paligemma\", result=caption, resolution_wh=(w, h), classes=CLASSES\n",
    "        )\n",
    "        image = sv.BoundingBoxAnnotator().annotate(image, detections)\n",
    "        image = sv.LabelAnnotator().annotate(image, detections)\n",
    "    except:\n",
    "        print(caption)\n",
    "    return f\"\"\"\n",
    "<div style=\"display: inline-flex; align-items: center; justify-content: center;\">\n",
    "    <img style=\"width:128px; height:128px;\" src=\"{render_inline(image, resize=(64,64))}\" />\n",
    "    <p style=\"width:256px; margin:10px; font-size:small;\">{html.escape(caption)}</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "html_out = \"\"\n",
    "for idx, example in zip(range(4), train_data_iterator()):\n",
    "    caption = postprocess_tokens(example[\"text\"])  # detokenize model input.\n",
    "    caption = split_and_keep_second_part(caption)\n",
    "    html_out += render_example(example[\"image\"], caption)\n",
    "\n",
    "print(\"Training examples\")\n",
    "display(HTML(html_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main update_fn using a simple stochastic gradient descent (SGD).\n",
    "@functools.partial(jax.jit, donate_argnums=(0,))\n",
    "def update_fn(params, batch, learning_rate):\n",
    "    imgs, txts, mask_ar = batch[\"image\"], batch[\"text\"], batch[\"mask_ar\"]\n",
    "\n",
    "    def loss_fn(params):\n",
    "        text_logits, _ = model.apply(\n",
    "            {\"params\": params}, imgs, txts[:, :-1], mask_ar[:, :-1], train=True\n",
    "        )\n",
    "        logp = jax.nn.log_softmax(text_logits, axis=-1)\n",
    "\n",
    "        # The model takes as input txts[:, :-1] but the loss is defined as predicting\n",
    "        # next tokens txts[:, 1:]. Additionally, mask_loss[:, 1:] indicates which tokens\n",
    "        # are part of the loss (e.g. prefix and padded tokens are not included).\n",
    "        mask_loss = batch[\"mask_loss\"][:, 1:]\n",
    "        targets = jax.nn.one_hot(txts[:, 1:], text_logits.shape[-1])\n",
    "\n",
    "        # Compute the loss per example. i.e. the mean of per token pplx.\n",
    "        # Since each example has a different number of tokens we normalize it.\n",
    "        token_pplx = jnp.sum(logp * targets, axis=-1)  # sum across vocab_size.\n",
    "        example_loss = -jnp.sum(token_pplx * mask_loss, axis=-1)  # sum across seq_len.\n",
    "        example_loss /= jnp.clip(jnp.sum(mask_loss, -1), 1)  # weight by num of tokens.\n",
    "\n",
    "        # batch_loss: mean of per example loss.\n",
    "        return jnp.mean(example_loss)\n",
    "\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "\n",
    "    # Apply gradients to trainable params using SGD.\n",
    "    def apply_grad(param, gradient, trainable):\n",
    "        if not trainable:\n",
    "            return param\n",
    "        return param - learning_rate * gradient\n",
    "\n",
    "    params = jax.tree_util.tree_map(apply_grad, params, grads, trainable_mask)\n",
    "\n",
    "    return params, loss\n",
    "\n",
    "\n",
    "# Evaluation/inference loop.\n",
    "def make_predictions(\n",
    "    data_iterator, *, num_examples=None, batch_size=4, seqlen=SEQLEN, sampler=\"greedy\"\n",
    "):\n",
    "    outputs = []\n",
    "    while True:\n",
    "        # Construct a list of examples in the batch.\n",
    "        examples = []\n",
    "        try:\n",
    "            for _ in range(batch_size):\n",
    "                examples.append(next(data_iterator))\n",
    "                examples[-1][\"_mask\"] = np.array(True)  # Indicates true example.\n",
    "        except StopIteration:\n",
    "            if len(examples) == 0:\n",
    "                return outputs\n",
    "\n",
    "        # Not enough examples to complete a batch. Pad by repeating last example.\n",
    "        while len(examples) % batch_size:\n",
    "            examples.append(dict(examples[-1]))\n",
    "            examples[-1][\"_mask\"] = np.array(False)  # Indicates padding example.\n",
    "\n",
    "        # Convert list of examples into a dict of np.arrays and load onto devices.\n",
    "        batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n",
    "        batch = big_vision.utils.reshard(batch, data_sharding)\n",
    "\n",
    "        # Make model predictions\n",
    "        tokens = decode(\n",
    "            {\"params\": params}, batch=batch, max_decode_len=seqlen, sampler=sampler\n",
    "        )\n",
    "\n",
    "        # Fetch model predictions to device and detokenize.\n",
    "        tokens, mask = jax.device_get((tokens, batch[\"_mask\"]))\n",
    "        tokens = tokens[mask]  # remove padding examples.\n",
    "        labels = [postprocess_tokens(e[\"label\"]) for e in examples]\n",
    "        responses = [postprocess_tokens(t) for t in tokens]\n",
    "\n",
    "        # Append to html output.\n",
    "        for example, label, response in zip(examples, labels, responses):\n",
    "            outputs.append((example[\"image\"], label, response))\n",
    "            if num_examples and len(outputs) >= num_examples:\n",
    "                return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_out = \"\"\n",
    "for image, _, caption in make_predictions(\n",
    "    validation_data_iterator(), num_examples=4, batch_size=4\n",
    "):\n",
    "    html_out += render_example(image, caption)\n",
    "display(HTML(html_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a short training loop with cosine learning rate schedule.\n",
    "#\n",
    "# Note: the first step can be quite slow on some machines (up to several minutes)\n",
    "# due to XLA compilation of the jax.jit'd function.\n",
    "#\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "TRAIN_EXAMPLES = 20_000\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "TRAIN_STEPS = TRAIN_EXAMPLES // BATCH_SIZE\n",
    "EVAL_STEPS = TRAIN_STEPS // 8\n",
    "\n",
    "train_data_it = train_data_iterator()\n",
    "\n",
    "sched_fn = big_vision.utils.create_learning_rate_schedule(\n",
    "    total_steps=TRAIN_STEPS + 1,\n",
    "    base=LEARNING_RATE,\n",
    "    decay_type=\"cosine\",\n",
    "    warmup_percent=0.10,\n",
    ")\n",
    "\n",
    "for step in range(1, TRAIN_STEPS + 1):\n",
    "    # Make list of N training examples.\n",
    "    examples = [next(train_data_it) for _ in range(BATCH_SIZE)]\n",
    "\n",
    "    # Convert list of examples into a dict of np.arrays and load onto devices.\n",
    "    batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n",
    "    batch = big_vision.utils.reshard(batch, data_sharding)\n",
    "\n",
    "    # Training step and report training loss\n",
    "    learning_rate = sched_fn(step)\n",
    "    params, loss = update_fn(params, batch, learning_rate)\n",
    "\n",
    "    loss = jax.device_get(loss)\n",
    "    print(\n",
    "        f\"step: {step:2d}/{TRAIN_STEPS:2d}   lr: {learning_rate:.5f}   loss: {loss:.4f}\"\n",
    "    )\n",
    "\n",
    "    if (step % EVAL_STEPS) == 0:\n",
    "        print(f\"Model predictions at step {step}\")\n",
    "        html_out = \"\"\n",
    "        for image, _, caption in make_predictions(\n",
    "            validation_data_iterator(), num_examples=4, batch_size=4\n",
    "        ):\n",
    "            html_out += render_example(image, caption)\n",
    "        display(HTML(html_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualize results\n",
    "html_out = \"\"\n",
    "for image, _, caption in make_predictions(\n",
    "    validation_data_iterator(), num_examples=16, batch_size=8\n",
    "):\n",
    "    html_out += render_example(image, caption)\n",
    "display(HTML(html_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Collect predictions\n",
    "targets = []\n",
    "predictions = []\n",
    "\n",
    "for image, label, prediction in make_predictions(\n",
    "    validation_data_iterator(), num_examples=512, batch_size=8\n",
    "):\n",
    "    h, w, _ = image.shape\n",
    "    target = sv.Detections.from_lmm(\n",
    "        lmm=\"paligemma\", result=label, resolution_wh=(w, h), classes=CLASSES\n",
    "    )\n",
    "    targets.append(target)\n",
    "    prediction = sv.Detections.from_lmm(\n",
    "        lmm=\"paligemma\", result=prediction, resolution_wh=(w, h), classes=CLASSES\n",
    "    )\n",
    "    prediction.confidence = np.ones(len(prediction))\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mAP\n",
    "mean_average_precision = sv.MeanAveragePrecision.from_detections(\n",
    "    predictions=predictions,\n",
    "    targets=targets,\n",
    ")\n",
    "\n",
    "print(f\"map50_95: {mean_average_precision.map50_95:.2f}\")\n",
    "print(f\"map50: {mean_average_precision.map50:.2f}\")\n",
    "print(f\"map75: {mean_average_precision.map75:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = sv.ConfusionMatrix.from_detections(\n",
    "    predictions=predictions, targets=targets, classes=CLASSES\n",
    ")\n",
    "\n",
    "_ = confusion_matrix.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "TARGET_MODEL_DIR = f\"{dataset.location}/model\"\n",
    "TARGET_MODEL_PATH = f\"{TARGET_MODEL_DIR}/paligemma-3b-pt-224.f16.npz\"\n",
    "\n",
    "os.makedirs(TARGET_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "flat, _ = big_vision.utils.tree_flatten_with_names(params)\n",
    "with open(TARGET_MODEL_PATH, \"wb\") as f:\n",
    "    np.savez(f, **{k: v for k, v in flat})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
